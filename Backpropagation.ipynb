{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "[reference](http://neuralnetworksanddeeplearning.com/chap2.html)\n",
    "\n",
    "Say cost function is C , the quadratic cost has the form\n",
    "\n",
    "$$ C = \\frac{1}{2n} \\sum_x || y(x) - a(x)^L ||^2 $$\n",
    "\n",
    "where n is the number of training examples. So, cost for single example $C_x = \\frac{1}{2} || y- a^L||^2$.\n",
    "\n",
    "The goal of backpropagation is the partial derivative $\\partial C / \\partial w$ and $\\partial C / \\partial b$ of the cost function $C$ with respect to any weight $w$ or bias $b$ in the network\n",
    "\n",
    "\n",
    "We define the error $\\delta_j^l$ of neuron $j$ in layer $l$ by\n",
    "\n",
    "$$\\delta_j^l = \\frac{\\partial C}{\\partial z_j^l}$$\n",
    "\n",
    "**An Equation for the error in the output layer L**, $\\delta^L$: The components of $\\delta^L$ are given by\n",
    "\n",
    "$$ \\delta_j^L = \\frac{\\delta C}{\\delta a_j^L} \\sigma' (z_j^L) $$\n",
    "\n",
    "The first term on the right measures how fast the cost is chaning as a function of the $j$-th output activation. Second term on the right measures how fast the activation function $\\sigma$ is chaning at $z_j^L$.\n",
    "\n",
    "It will be computed very easily. If we are using quadratic cost function that we defined above\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial a_j^L} = (a_j - y_j), $$\n",
    "\n",
    "So, \n",
    "\n",
    "$$\\delta^L = (a^L-y) \\odot \\sigma'(z^L).$$\n",
    "\n",
    "**An Equation for the error $\\delta^l$ in terms of the error in the next layer, $\\delta^{l+1}**, \n",
    "\n",
    "$$ \\delta^l = ((w^{l+1})^T \\delta^{l+1})) \\odot \\sigma'(z^l)$$\n",
    "\n",
    "**An equation for the rate of change of the cost with respect to any bias in the network**\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial b_j^l} = \\delta_j^l $$\n",
    "\n",
    "Because the error is exactly equal to the rate of change $\\frac{\\partial C}{\\partial b_j^l}$,\n",
    "\n",
    "**An equation for the rate of change of the cost with respect to any weight in the network**\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l $$\n",
    "\n",
    "where $k$ is the index in output layer, $j$ is the index in the input layer which is in previous layer.\n",
    "\n",
    "So, we can re-write \n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w} = a_{in} \\delta_{out} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* See the code below for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Network():\n",
    "#...\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\"\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw \n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb \n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the\n",
    "        gradient for the cost function C_x.  \"nabla_b\" and\n",
    "        \"nabla_w\" are layer-by-layer lists of numpy arrays, similar\n",
    "        to \"self.biases\" and \"self.weights\".\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        \n",
    "        ## belows for feedforward algorithm\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid_vec(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime_vec(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            spv = sigmoid_prime_vec(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * spv\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "#...\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y) \n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "sigmoid_vec = np.vectorize(sigmoid)\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
